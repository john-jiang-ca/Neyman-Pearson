% This is for chapter 3
\section{The ROC Surface of Modified Extended Neyman Pearson Test}

\subsection{Modified Receiver of Characteristic of MENP test}

The ROC surface of MENP test (M-ROC) depicts the relationship between $P_d$ and $c_1, c_2, ..., c_M$. On one hand, it  illustrates the largest $P_d$ can be achieved under the constraint $\mathbf{P}_{f} \leq \mathbf{c}$, on the other hand, it provides the range of $\mathbf{c}$ for a given $P_d$.
Points $(P_d, c_1, c_2, ..., c_M)$ on M-ROC  surface can be divided into two types: 
\\1. Those with $[c_1, c_2, ..., c_M] \in \alpha^+$. Define this set of points as $M_0$; 
\\2. Those with $[c_1, c_2, ..., c_M] \notin \alpha^+$. Define this set of points as $M_1$. 

Obviously points in $M_0$ can be achieved by MENP (\rmnum{1}) and points in $M_1$ can  be achieved by MENP (\rmnum{2}). Next we consider a property of M-ROC surface.

\noindent \textbf{Property 1}
\noindent \textit{
  \noindent Assume hypotheses given as:
}
\begin{equation}
  \begin{split}
	H_0:\;\;\;\;\;\;&X \sim f_0(x)\\
	H_i:\;\;\;\;\;\;&X \sim f_i(x)\;\;\;\;i=1, 2, \cdots, M
  \end{split}
\end{equation}
\textit{
  define $g(x) = \frac{\sum_{i=1}^{M}k_if_i(x)}{f_0(x)}$ and let $F_i(x)$ represent the CDF of hypothesis $H_i$ ($i = 1, ..., M$). If $g(x)$ is a monotonically increasing function of $x$ for any non-negative $k_i\;\;(i = 1, ..., M)$ and $F_i(x)$ is monotonically increasing function, then we have:}

  \textit{(1)The region achieved by ENP test with non-negative parameters degenerates to a curve.}

  \textit{(2)For a specific false alarm constraints $P_{f_i} \leq c_i\;\;(i = 1, ..., M)$, the decision rule for MENP test is $x \substack{\bar{H}_0 \\ \geq \\ < \\H_0} x_0$, where $x_0 = \min\{F_1^{-1}(c_1), ..., F_M^{-1}(c_M)\}$.}

  \textit{(3)The expression of $P_d$, $P_{f_1}$, ..., $P_{f_M}$ can be written as}
  \begin{equation}
	\label{equ: chi pd}
	\begin{split}
	  P_d & = F_0(x_0)\\
	  P_{f_i} & = F_i(x_0)\;\;\;\;i = 1, 2, \cdots, M
	\end{split}
  \end{equation}

  \noindent \textbf{PROOF}
  According to the definition, $M_0$ is the region achieved by ENP test with $k_i \geq 0 (i=1, ..., M)$. The ENP decision rule is
$	\frac{\sum_{i=1}^{M}k_if_i(x)}{f_0(x)} \substack{\bar{H}_0 \\\geq\\< \\H_0}1$.
  From the condition $g(x) = \frac{\sum_{i=1}^{M}k_if_i(x)}{f_0(x)} $, this equation  can be written as 
$	g(x)\substack{\bar{H}_0 \\\geq\\< \\H_0}1$.
  Since $g(x)$ is a monotonically increasing function with $x$, hence $g^{-1}(x)$ exists and the decision rule  can be written in form of 
  \begin{equation}
	\label{dec: x0}
	x\substack{\bar{H}_0 \\\geq\\< \\H_0}x_0\,,
  \end{equation}
  where $x_0 = g^{-1}(1)$.
  Under decision rule \eqref{dec: x0}, the expression of $P_d$, $P_{f_1}$, ..., $P_{f_M}$ is: 
  \begin{equation}
	\begin{split}
	  \label{equ: pd under x0}
	  P_d &= \text{Pr}(X \leq x_0 | H_0) = F_0(x_0)\\
	  P_{f_i} &= \text{Pr}(X \leq x_0 | H_i) = F_i(x_0)\;\;\;\;i=1, 2, \cdots, M
	\end{split}
  \end{equation}
  where $F_0$, $F_1$, ..., $F_M$ are the CDFs of $X$ under $H_0$, $H_1$, ..., $H_M$. From \eqref{equ: pd under x0}, $P_{f_1}$ determines $x_0$ that in turn determines $P_{f_2}$, ..., $P_{f_M}$, $P_d$. Hence for a given $P_{f_1}$, there is only one corresponding $P_{f_i} (i= 1, ..., M)$. Hence the ROC surface with $k_i \geq 0 (i = 1, ..., M)$ degenerates to a curve in this case.

  Furthermore, \eqref{dec: x0}  implies for an ENP decision rule with non-negative parameters, there exists an $x_0$ such that the ENP decision rule can be written in form of \eqref{dec: x0}. 

  Next we consider the optical decision rule $\delta^\ast$ of \eqref{equ: problemstate}. 
  From \textbf{Lemma 1} we know $\delta^\ast$ is an ENP test with non-negative parameters, hence according to our previous discussion there exists an $x_0$ such that $\delta^\ast$ can be written in form of \eqref{dec: x0}  
  and the expression of $P_d$ and $\mathbf{P}_f$ are given in \eqref{equ: pd under x0}. 
  In the following, we will determine the value of $x_0$. 
  As $P_d$ is an increasing function with respect to $x_0$, in order to acquire the maximum $P_d$ we need to achieve the largest $x_0$ while keeping 
$	P_{f_i} \leq c_i$.
Substitute \eqref{equ: pd under x0} into $P_{f_i} \leq c_i$, we have
  \begin{equation}
	\label{1125a1}
	\;\;\;\; x_0 \leq F^{-1}_{i}(c_i) \;\;\;\;(i=1, 2, ..., M)\,.
  \end{equation}
  \eqref{1125a1} can be fulfilled only if $x_0$ is 
  smaller or equal to the minimum of $F^{-1}_{i}(c_i)$ ($i=1, 2, ..., M$). This implies the largest $x_0$ can be achieved is $\min\{F_1^{-1}(c_1), F_2^{-1}(c_2), ..., F_M^{-1}(c_M)\}$, i.e.
$	x_0 = \min(F_1^{-1}(c_1), F_2^{-1}(c_2), ..., F_M^{-1}(c_M))$.

%==================================================================================================
% The Gaussian Case
\subsection{MROC Surface under Gaussian Hypotheses}
In the following two examples, we compute the M-ROC under Gaussian Hypotheses. 

\noindent \textbf{Example 1:}
Assume three hypotheses given as \eqref{2015jan29a2}. We goal is using MENP to detect hypothesis $H_0$ against $\bar{H}_0$ and get the M-ROC surface.  
To form the M-ROC surface, we first consider points belong to $M_0$.
From previous discussion, we can see when $(P_d, c_1, c_2) \in M_0$, there exists non-negative $\mathbf{k}$ such that by using decision rule \eqref{equ: decision rule},
we have 
\begin{equation}
\begin{split}
\label{1125c0}
&P_d = \int_{-\infty}^{\infty} u(f_0(x) - \sum_{j=1}^{2}k_jf_j(x)) f_0(x)\mathrm{d}x    \,, \\
&P_{f_i} = \int_{-\infty}^{\infty} u(f_0(x) - \sum_{j=1}^{2}k_jf_j(x)) f_i(x) \mathrm{d}x = c_i\;\;\;\;\;    i=1, 2\,.
\end{split}
\end{equation}
We use Matlab to compute the M-ROC for region $M_0$. The values of $k_1$ and $k_2$ range from $0$ to $100$ in steps of $0.01$. Substituting the value of $k_1$ and $k_2$ into \eqref{1125c0}, results in the corresponding $P_d$ $P_{f_1}$ and $P_{f_2}$.  The set $M_0$ is illustrated in Figure \ref{pic: surface for m0 gaussian}. Figure \ref{pic: contour for m0 gaussian} presents the projection of Figure \ref{pic: surface for m0 gaussian} on the $c_1, c_2$ plane.

In Figure \ref{pic: contour for m0 gaussian} $N_0$ is the projection of $M_0$ on the $c_1, c_2$ plane. Since $M_0$ is the set of points with $[c_1, c_2] \in \alpha^+$, $N_0$ is the set of points belonging to $\alpha^+$.
Define curve $L_1$ as the set of points such that 
\[
\{ (c_1, c_2) \in L_1 | (c_1, c_2) \in {N}_0 \;\;\text{and} \;\;(c_1, c_2+\epsilon)\notin {N}_0 \;\;\;\;\text{for any positive $\epsilon$} \}\,.
\]
Define curve $L_2$ as the set of points such that 
\[
\{ (c_1, c_2) \in L_2 | (c_1, c_2) \in {N}_0 \;\;\text{and} \;\;(c_1 + \epsilon, c_2)\notin {N}_0 \;\;\;\;\text{for any positive $\epsilon$} \}\,.
\]
Let $N_1$ denote the region enclosed by line $c_1 = 0$, $c_2$; line $c_1$, $c_2 = 1$ and curve $L_1$.
Let $N_2$ denote the region enclosed by line $c_1 = 1$, $c_2$; line $c_1$, $c_2 = 0$ and curve $L_2$.
The regions of $N_0$ $N_1$ and $N_2$ are shown in Figure \ref{pic: contour for m0 gaussian}.

In the following, we present the decision rule for points belong to region $N_1$ and $N_2$.

\noindent \textbf{Property 2:}
\textit{\\(1) All points belonging to region $N_1$ or curve $L_1$, if they have the same $c_1$, they have the same decision rule and same $P_d$.
\\(2) All points belonging to region $N_2$ or curve $L_2$, if they have the same $c_2$, they have the same decision rule and same $P_d$.
}

\noindent \textbf{PROOF}
Recall $F(\mathbf{c})$ is defined as the largest $P_d$ that can be achieved under constraint $\mathbf{P}_f = \mathbf{c}$ and 
       $G(\mathbf{c})$ is defined as the largest $P_d$ can be achieved under constraint $\mathbf{P}_f \leq \mathbf{c}$.
Firstly we will show $F(\mathbf{c}) = G(\mathbf{c}) $ when $\mathbf{c} \in \alpha^+$.

According to the definition of $\alpha^+$, for a point $(c_1, c_2) \in \alpha^+$, there exists a decision rule 
$
\delta:\;\;\;\;f_0(x) \substack{H_0 \\ \geq \\ < \\ \bar{H}_0} k_1f_1(x) + k_2f_2(x) \;\;\;\;(k_1, k_2 \geq 0)
$
such that under decision rule $\delta$, we have 
$
\mathbf{P}_{f}(\delta) = \mathbf{c}
$, and from ENP Lemma (\rmnum{2}) we have  $
P_d(\delta) = F(\mathbf{c})
$. Since $k_1, k_2 \geq 0$, according to ENP Lemma (\rmnum{3}), $\delta$ also achieve the largest $P_d$ while keeping $\mathbf{P}_f \leq \mathbf{c}$, i.e. 
$
 P_d(\delta) =G(\mathbf{c}) 
$. 
Thus we can see $F(\mathbf{c}) = G(\mathbf{c})$ for $\mathbf{c} \in \alpha^+$.
In Section 2.2.2,  we have shown $G(\mathbf{c})$ is a non-decreasing function for each variable of $\mathbf{c}$, hence it can be concluded that $F(\mathbf{c})$ is a non-decreasing function for each variable of $\mathbf{c}$ on set $\alpha^+$.

As it is shown in Figure \ref{pic: contour for m0 gaussian}, A is a point in region ${N}_1$ with coordinates $(c_1, c_2) = (c_{1_A}, c_{2_A})$. In the following we will derive its optimal decision rule through MENP test. By optimal decision rule, we mean   it achieves the largest $P_d$ while keeping $\mathbf{P}_f \leq \mathbf{c}$.  
As point A does not belong to $\alpha^+$, we should use MNEP (\rmnum{2}) to get its decision rule. To do it, the first step is to determine set $\mathcal{C}$, which is the intersection of $\mathcal{A}_c$ and $\alpha^+$. In this case, set $\mathcal{C}$ is the area enclosed by $L_1$, $L_2$ and $c_1=0.2$, $c_2$.
After we have $\mathcal{C}$, we need to find $\mathbf{c}^0 \in \mathcal{C}$ such that it maximum $F(\mathbf{c})$ among all $\mathbf{c} \in \mathcal{C}$.
Figure \ref{pic: contour for m0 gaussian}  shows that point B (with coordinate $(c_{1_B}, c_{2_B})$ where $c_{1_B} = c_{1_A}$) has the largest $c_1$ $c_2$ components among all $\mathbf{c} \in \mathcal{C}$. Since $F(\mathbf{c})$ is a non-decreasing function for $c_1$ and $c_2$ on set $\alpha^+$, it is easy to see
\begin{equation}
  \max_{\mathbf{c} \in \mathcal{C}}\;\;\;\;F(\mathbf{c}) = F(c_{1_B}, c_{2_B})\,.
  \label{2015apr30a0}
\end{equation}
 As point B is in set $\mathcal{C}$, there exists decision rule
 $
\delta^\ast:\;\;\;\;f_0(x) \substack{H_0 \\ \geq \\ < \\ \bar{H}_0} k_1^\ast f_1(x) + k_2^\ast f_2(x) \;\;\;\;(k_1^\ast, k_2^\ast \geq 0)
 $
 such that under $\delta^\ast$ we have  $P_{f_1}(\delta^\ast) = c_{1_B}$ and  $P_{f_2}(\delta^\ast) = c_{2_B}$. 
 From ENP (\rmnum{3}), it is easy to see $\delta^\ast$ is the optimal decision rule for point B.
 Since $(c_{1_B}, c_{2_B})$ satisfies \eqref{2015apr30a0}, by using MENP (\rmnum{2}), we know $\delta^\ast$ is also the optimal decision rule for point A. Thus we can see the optimal decision rule for point A and point B are the same. Since point A lies in region $N_1$, point B lies in region $L_1$ and $c_{1_A} = c_{1_B}$, we can conclude for two points respectively belongs to $N_1$ and $L_1$, if they have the same $c_1$ component, they have the same decision rule and same $P_d$. 

Furthermore, we can see as long as A is in region $N_1$ its decision rule only depends on the value of $c_{1}$.  In other words, as long as A is in region $N_1$, with $c_1 = c_{1_A}$ fixed,  when $c_{2}$ changes, its optimal decision rule and $P_d$ remain the same. Hence we can conclude all points belonging to region $N_1$ or curve $L_1$, if they have the same $c_1$, they have the same decision rule and same $P_d$.

In the same way, we can prove: For points belong to region $N_2$ or curve $L_2$, if they have the same $c_2$ value, they have the same decision rule and same $P_d$.

Q.E.D

Since we have computed $P_d$ for $(c_1, c_2) \in N_0$ and curves $L_1$ and $L_2$ belong to $N_0$, we can get $P_d$ for $(c_1, c_2)$ belongs to $N_1$ and $N_2$ through \textbf{Property 2}. M-ROC surface for this example is given in Figure \ref{pic: LJS}. For easily demonstration, the whole M-ROC is divided into three regions ($M_1$, $M_0$ and $M_2$).  
The projection of $M_1$ on $c_1-c_2$ plane is set $N_1$, and the projection of $M_2$ on $c_1-c_2$ plane is set $N_2$.  

%==================================================================================================
% Chi-Square Case
\subsection{MROC Surface under Chi-Square Hypotheses}
\noindent\textbf{Example:}

Assume $M+1$ hypotheses  given as:
\begin{equation}
  \label{equ: Chisquare Hypothesis}
  \begin{split}
    H_0:\;\;\;\;\;\;\;\;&\frac{X}{\sigma_0^2} \sim \mathcal{X}^2(2N)\\
    H_i:\;\;\;\;\;\;\;\;&\frac{X}{\sigma_i^2} \sim \mathcal{X}^2(2N)\;\;\;\;i=1, 2, \cdots, M
  \end{split}
\end{equation}
where $\mathcal{X}^2(2N)$ is the Chi-square distribution with  $2N$ degree freedom($N$ is an integer, $\sigma_0^2 < \sigma_1^2, ..., \sigma_M^2$ and $\sigma_i^2 \neq \sigma_j^2$ if $i \neq j$). By a random variable transformation space \cite{mark2011probability}, we can get the PDFs for the hypotheses:

\def \CHISQU[#1]{\frac{1}{#1 2^N\Gamma(N)}\left(\frac{x}{#1}\right)^{N-1}\exp\left(-\frac{x}{2#1}\right)\\}
\begin{equation}
  \label{equ: Chisquare Distribution}
  \begin{split}
    H_0:\;\;\;\;\;\;\;\;&f_0(x) = \CHISQU[\sigma_0^2]\\
    H_i:\;\;\;\;\;\;\;\;&f_i(x) = \CHISQU[\sigma_i^2]
  \end{split}
\end{equation}

In the following, we will prove that in this example the region achieved by ENP test with $\mathbf{k}$ degenerates to a curve.

Consider
 $ g(x) = \frac{\sum_{i=1}^{M}k_if_i(x)}{f_0(x)} \;\;\;\;k_i \geq 0$. 
Substituting $f_i(x) (i=1, ..., M)$ from \eqref{equ: Chisquare Distribution} into $g(x)$, we get:

\begin{equation}
  \label{equ: decision rule chi 1}
g(x) = \sum_{i=1}^{M}k_i'\exp{(\frac{1}{2\sigma_0^2} - \frac{1}{2\sigma_i^2})x} 
\end{equation}
where $k_i' = k_i(\frac{\sigma_0}{\sigma_i})^{2N}, i= 1, ..., M$. Define $p_i = \frac{1}{2\sigma_0^2} - \frac{1}{2\sigma_i^2}, i=1, ..., M$. Hence $g(x) =  \sum_{i=1}^{M}k_i'\exp{(p_ix)}$.

The parameters $k_i' (i=1, ..., M)$ are always non-negative when $k_i (i=1, ..., M)$ are such, and from 
 the condition $\sigma_0^2 \leq \sigma_i^2 (i=1, ..., M)$ we can conclude $p_i (i=1, ..., M)$ are positive. Hence $g(x)$ is a monotonically increasing function with $x$. From \textbf{Property 1}, we have that the region achieved by ENP test with $k_i (i = 1, ..., M)$ degenerates to a curve. For a specific $\mathbf{c}$, the decision rule is 
$  x \substack{\bar{H}_0 \\ \geq \\ < \\ H_0} x_0\,,$
where $x_0 = \min\{F_1^{-1}(c_1), ..., F_M^{-1}(c_M)\}$,
and the corresponding $P_d = F_0(x_0)$. 

For the case when $M=2$, $\sigma_0^2 = 0.1$, $\sigma_1^2 = 0.05$, $\sigma_2^2 = 0.15$ and $N=20$, the M-ROC surface is illustrated in 
Figure \ref{pic: LJS for chisquare}. 



