% determine the decision rule

\section{Determining the decision rule under the MENP framework}
In the context of spectrum sensing, the lookup table $T_1$ can be pre-computed  before the detector is deployed. When the probability of false alarm constraint(s) is(are) set, the detector utilizes $T_1$ to get the MENP decision  rule parameters accordingly. The size of $T_1$ depends on the value of $M$, the range and the step of $k_i$.  Assume the value of $k_i$ ranges from $0$ to $\Gamma$ with step $\tau_0$. As $T_1$ is an exhaustive lookup table on the grid of $k_1, ..., k_M$, it has $(\frac{\Gamma}{\tau_0})^M$ items. Since an increase in $M$ leads to an exponential increase in the size of $T_1$, this method is not adequate when $M$ is large. 

In \cite{zhang1999design, zhang2000efficient}, Qian proposed an algorithm to solve this issue. 
Qian states that his algorithm generates the optimal solution for \eqref{equ: problemstate} when $\mathbf{c} \in (0, 1)^M$.
However, he only provides the proof for the situation when $\mathbf{c} \in \alpha^+$. For the case when $\mathbf{c} \notin \alpha^+$, he does not prove the solution generated by his algorithm is optimal or not.  
% Qian showed that if $\mathbf{c} \in \alpha^+$, the solution generated by the algorithm is optimal; 
% for the case when $\mathbf{c} \notin \alpha^+$, the algorithm can provide a feasible solution. There is no further discussion for whether or not this feasible solution is optimal for \eqref{equ: problemstate} in his original work.  
In the following, we will show that the algorithm generate the optimal solution for \eqref{equ: problemstate} when $\mathbf{c} \in (0, 1)^M$, no matter whether or not $\mathbf{c}$ belongs to $\alpha^+$. 
From \cite{zhang1999design, zhang2000efficient}, we know that the algorithm can iteratively refine $k_i$ ($i=1, ..., M$) and generate an ENP test with non-negative parameters such that among all ENP tests with non-negative parameters it achieves the largest $P_d$ while keeping $P_{f_i}$ below the prescribed constraints. 
In section 2.4, we showed that no matter $\mathbf{c} \in \alpha^+$ or not, the optimal decision rule for \eqref{equ: problemstate} is an ENP decision rule  with non-negative parameters such that: (a) its $P_{f_i}$ is smaller or equal to its associated constraints; (b) it maximizes $P_d$ among all ENP decision rule with non-negative parameters satisfying (a).  
It is clear that the solution generated by Qian's algorithm satisfies condition (a) and (b).
From the above discussion, we can see the  algorithm approached by Qian in \cite{zhang1999design, zhang2000efficient} can generate the solution for \eqref{equ: problemstate} when $\mathbf{c} \in (0, 1)^M$.

The algorithm is summarized as follows. Let $t = \sum_{i=1}^{M}k_i$ and $\omega_i = \frac{k_i}{t}$ ($i=1, 2, ..., M$).
Let $\bom^T = [\omega_1, \cdots, \omega_M]$. 
In the algorithm, $\bom$ is termed the weight of the decision rule and $t$ is termed the threshold of the decision rule. Since only non-negative $k_i$ are considered, both $\omega_i, t$ are greater or equal to zero. The ENP test can be written in form of 
\begin{equation}
\label{qian dec}
f_0(x) \substack{H_0 \\ \geq \\ < \\ \bar{H}_0} t\sum_{i=1}^{M}\omega_if_i(x)\,.
\end{equation}

%Let $P_d$ and $\mathbf{P}_f$ denote the probability of detection and probability of false alarm of decision rule \eqref{qian dec} respectively. 
From \cite{zhang2000efficient} we have that with $\omega_i$ ($i=1, 2, ..., M$) fixed, the probability of detection ($P_d$) and probability of false alarm ($\mathbf{P}_f$) under decision rule \eqref{qian dec} are continuous decreasing function with respect to $t$. 
Moreover, when $t$ increases from $0$ to positive infinity, $P_d$ and $P_{f_i}$ decrease from $1$ to $0$.

The algorithm contains four steps: (\rmnum{1}) initialization; (\rmnum{2}) weight adjustment; (\rmnum{3}) threshold adjustment; (\rmnum{4}) tolerance checking.

In initialization, $\omega_i$ ($i=1, 2, ..., M$)  is assigned arbitrary values on condition that $\sum_{i=1}^{M}\omega_i = 1$. Using this $\bom$, the algorithm find the initial value of threshold $t$ such that among all possible $t$, it maximize $P_d$ while keeping $P_{f_i} \leq c_i$. 
To do this, the algorithm iteratively decreases $t$ from positive infinity until a probability of false alarm is equal to its associated constraint and other probability of false alarms are either below or equal to their associated constraints.   
After the initialization, we derived a feasible decision rule for \eqref{equ: problemstate}. However this may not be the optimal solution for \eqref{equ: problemstate}. This is because the value of $\bom$ is randomly set and it is possible that when $\bom$ changes, the associated $P_d$ increases while keeping $\mathbf{P}_f$ is no larger than $\mathbf{c}$. Hence in order to derive the optimal decision rule, the algorithm will further adjust the weights and thresholds in the following steps. 

After initialization, the algorithm will begin the weights adjustment process. 
Assume before this process, the weight is $\bom_0$, the threshold is $t_0$ and the associated probability of detection is $P_d^0$.
In this step, with the probability of detection fixed, the algorithm modifies the value of $\bom$ and $t$ until $\mathbf{P}_f$ is strictly below $ \mathbf{c}$, i.e. using the modified decision rule, we have $\mathbf{P}_f < \mathbf{c}$ and $P_d = P_d^0$. (This step would make $\mathbf{P}_f < \mathbf{c}$ holds. Then in the next step, the $P_d$ can be further increased while keeping $\mathbf{P}_f \leq \mathbf{c}$ \cite{zhang2000efficient}). 
Let $\delta\bom$ denote the perturbations of $\bom_0$ and let $\bom'$ denote the modified weight, i.e. $\bom' = \bom_0 + \delta\bom$.  It is shown in \cite{zhang2000efficient} that in order to make $\mathbf{P}_f < \mathbf{c}$ while keeping $P_d$ unchanged, $\bom'$ should ensure $\theta(\mathbf{P}_f, \mathbf{c})$ decrease ($\theta(\mathbf{A}, \mathbf{B})$ denotes the angle between vectors $\mathbf{A}$ and $\mathbf{B}$, i.e. 
$\cos (\theta(\mathbf{A}, \mathbf{B})) =$ 
$\frac{\mathbf{A}\bullet \mathbf{B}}{|\mathbf{A}||\mathbf{B}|}$,
here $|\mathbf{A}|$ and $|\mathbf{B}|$ denote the length of vector $\mathbf{A}$, $\mathbf{B}$ and $\mathbf{A}\bullet \mathbf{B}$ denotes the dot product of $\mathbf{A}$ and $\mathbf{B}$). To make $\theta(\mathbf{P}_f, \mathbf{c})$ decrease, \cite{zhang2000efficient} proved that the direction of $\delta\bom$ must be the same as
\begin{equation}
\label{direction}
\mathbf{D} - m_\mathbf{D}\bom_0
\end{equation}
where 
\[
\mathbf{D} = \mathbf{P}_f - \mathbf{c}\frac{|\mathbf{P}_f|^2}{\mathbf{c}^T\mathbf{P}_f}\,,
\]
and $m_{\mathbf{D}}$ represents the arithmetic sum of the elements of $\mathbf{D}$. Hence $\delta\bom$ can be written in form of 
\begin{equation}
\label{om expression}
\delta\bom = \lambda( \mathbf{D} - m_\mathbf{D}\bom_0) 
\end{equation}
where $\lambda$ is a value associated with the magnitude of $\delta\bom$.
According to \cite{zhang2000efficient}, a too large $\lambda$ may result in a  $\theta(\mathbf{P}_f, \mathbf{c})$ increase, which is undesirable, while a too small value can not decrease $\theta(\mathbf{P}_f, \mathbf{c})$ very much. The algorithm chooses the value of $\lambda$ by trial and error. To do this, the algorithm choose the arithmetic average of $\mathbf{P}_f$ as the initial value of $\lambda$ and compute the new weights $\bom'$ through \eqref{om expression}. 
If any element of $\bom'$ is negative, the negative element will be changed to zero, e.g. if $\omega_i < 0$, the algorithm will use $\omega_i = 0$. This is because a negative  $\omega_i$ is not considered in this method.  
Then the algorithm uses $\bom'$ to find the $t'$ such that under the new decision rule the probability of detection is equal to  $P_d^0$. To do this, with $\bom'$ fixed, the algorithm iteratively decreases the value of $t'$ from positive infinity until $P_d = P_d^0$ is satisfied. Under the new decision rule, if $\mathbf{P}_f$ is strictly below $\mathbf{c}$, the weight adjustment is finished and the algorithm will process to  threshold adjustment; otherwise, the algorithm reduces the value of $\lambda$ and recomputes $\bom'$ and $t'$ through \eqref{om expression} until $\mathbf{P}_f < \mathbf{c}$  is satisfied. 

 The algorithm performs the threshold adjustment process after weight adjustment. Since all probabilities of false alarm are strictly below its associated constraint, by appropriately revising the threshold, the probability of detection can be further improved while maintaining $\mathbf{P}_f \leq \mathbf{c}$. In this process, with $\omega_i$ ($i=1, 2, ..., M$) fixed, the algorithm iteratively decreases the value of $t$ to increase the value of $P_d$ until a probability of false alarm is equal to its associated constraint.  

The last step is tolerance checking. From \cite{zhang2000efficient} we have that the theoretical solution satisfies a false alarm probability constraint with equality if the associated weight is non-zero; a false alarm is strictly below the constraint if the associated weight is zero. 
Let $I$ be the set of subscripts such that $i \in I$ if and only if $\omega_i > 0$. 
Hence in the tolerance check, the algorithm checks whether $|c_i - P_{f_i}|$ ($i \in I$) is within a prescribed tolerance value . 
(Due to the accuracy limitation of numerical integration, for a given decision rule, the computer cannot provides an accurate value for its associated $\mathbf{P}_f$ and $P_d$. When $|c_i - P_{f_i}|$ are within the tolerance value, we treat them to be equal.) 
If so, it means the decision rule is closed enough to the theoretical solution  and the algorithm will output the threshold and weight (the theoretical solution of $k_i$, $i= 1, \cdots, M$, could be a real number, but since computers compute and store rational number, the computers can only output an approximate solution). If not the algorithm will start a new cycle of weight adjustment - threshold adjustment - tolerance checking process. 

For each cycle, let $P_d^0$ represents the probability of detection before the cycle starts, and let $P_d^1$ represents the probability of detection after the cycle ends. In the weight adjustment process, the weight and threshold are refined such that the probability of detection is fixed. In the threshold adjustment process, the algorithm revise the threshold such that the probability of detection increases. Hence we can see $P_d^0$ is strictly below $P_d^1$, i.e. after each cycle, the probability of detection increases. Let $P_d^\ast$ denote the theoretical solution. As long as $P_d^0 < P_d^\ast$  the algorithm can bring the probability of detection closer to the theoretical solution  through changing the threshold $t$ and weight $\bom$.

\subsection{An Example}
In the following we use an example to illustrate the operation of the algorithm. Assume three hypotheses given as

\begin{equation}
\label{equ: Gaussian Hypothesis}
\begin{split}
	H_0:\;\;\;\;\;\;\;\;&X \sim \mathcal{N}(-1,1)\\
    H_1:\;\;\;\;\;\;\;\;&X \sim \mathcal{N}(0,1)\\
    H_2:\;\;\;\;\;\;\;\;&X \sim \mathcal{N}(1,10)\,,
\end{split}
\end{equation}
where $\mathcal{N}(\mu, \epsilon^2)$ denotes a Gaussian PDF with mean $\mu$ and variance $\epsilon^2$.
The tolerance of the algorithm is set to $0.0001$. The initial weights of the algorithm are set to $\omega_1 = \omega_2 = 0.5$. 
Two cases will be considered in this part. In the first case we consider the situation when $\mathbf{c} \in \alpha^+$. In the second case we consider the case when $\mathbf{c} \notin \alpha^+$.

First we consider the case when $c_1 = 0.15$ and $c_2 = 0.2$.
After the initialization, the weights and threshold are set to $\omega_1 = \omega_2 = 0.5$, $t =2.3544$. 
Since $\omega_i = \frac{k_i}{t}$ ($i = 1, 2$), we have $k_1 = 1.1772$ and $k_2 = 1.1772$. 
The corresponding performance measures can be computed through \eqref{equ: pf and pd} and we get $P_{f_1}  = 0.1500$, $P_{f_2} = 0.1445$ and $P_d = 0.4520$. The algorithm performs $163$ iterations of weight threshold adjustments. In Figure \ref{fig: 2.3} the $(P_{f_1}, P_{f_2})$  points after each iteration are represented as `$\bullet$'. 
The $(P_{f_1}, P_{f_2})$ point of the initialization is plotted as  a `o' and the $(P_{f_1}, P_{f_2})$ point of the final solution is plotted as a `$\square$'. For each iteration, both $P_{f_1}$ and $P_{f_2}$ changed. But comparing to the change of $P_{f_2}$, the change of $P_{f_1}$ is much smaller. Hence visually $P_{f_1} $ does not change in Figure \ref{fig: 2.3}. 
In Figure \ref{fig: 2.2}, the change of $P_d$ with respect to the iteration times is depicted through a curve marked with stars.   
The dotted line in Figure \ref{fig: 2.2} represents the theoretical value of $P_d$ under constraint $\mathbf{P}_f \leq \mathbf{c}$. This value was calculated by the look-up table method approached in last section. We can see, after each cycle of weight-threshold adjustment, the corresponding $P_d$ converges to the theoretical solution. 

\begin{figure}[H]
\centering
\includegraphics[width = 12cm, height = 16cm]{2/152pf.eps}
\caption{Change of $P_{f_1}$, $P_{f_2}$ after each iteration.}
\label{fig: 2.3}
\end{figure}
\newpage
\begin{figure}[H]
\centering
\includegraphics[width = 12cm,  height = 18cm]{2/152pd.eps}
\caption{Change of $P_d$ after each iteration.}
\label{fig: 2.2}
\end{figure}
\newpage

The weight and threshold of the final solution are

\[
\begin{split}
\omega_1 = 0.9208\\
\omega_2 = 0.0792\\
t = 1.7889
\end{split}
\]
Since $k_1 = \omega_1 t$ and $k_2 = \omega_2 t$, we can see
\[
\begin{split}
k_1 &= 1.6474\\
k_2 &= 0.1416
\end{split}
\]
The performance measure of the decision rule are
\[
\begin{split}
P_{f_1} &= 0.1500\\
P_{f_2} &= 0.1999\\
P_d &= 0.4838
\end{split}
\]


We can see, $P_{f_1}$ is equal to its associated constraint $c_1$  while $P_{f_2}$ is $0.0001$ smaller than its associated  constraint $c_2$. 


Then we consider an ENP test for $c_1 = 0.2$ and $c_2 = 0.4$.
After the initiation, the weight and threshold are $\omega_1 = \omega_2 = 0.5$, $t =2.0278$, and the corresponding performance measures are $P_{f_1}  = 0.2000$, $P_{f_2} = 0.1720$ and $P_d = 0.5367$. The algorithm carries $14$ iterations of weight  threshold adjustment. The corresponding $(P_{f_1}, P_{f_2})$ are   
marked as `*' and connected by a dotted line in Figure \ref{fig: 2.4}. 
The $(P_{f_1}, P_{f_2})$ point of the initialization is plotted as  a `o' and the $(P_{f_1}, P_{f_2})$ point of the final solution are plotted as a `$\square$'. The change of $P_d$ after each iteration is plotted in Figure \ref{fig: 2.5}. 
The dotted line in Figure \ref{fig: 2.5} represents the theoretical probability of detection under constraint $\mathbf{P}_f \leq \mathbf{c}$. This value was calculated by the exhaustive search method approached last section.  


The weight and threshold of the final solution are
\[
\begin{split}
\omega_1 = 1.000\\
\omega_2 = 0.000\\
t = 1.4075
\end{split}
\]
Since $k_1 = \omega_1t$ and $k_2 = \omega_2t$, we can see
\[
\begin{split}
k_1 &= 1.4072\\
k_2 &= 0.000
\end{split}
\]
The performance measure of the decision rule are
\[
\begin{split}
P_{f_1} &= 0.2000\\
P_{f_2} &= 0.2799\\
P_d &= 0.5629
\end{split}
\]

\begin{figure}[H]
\centering
\includegraphics[width = 12cm , height = 16cm]{2/24pf.eps}
\caption{Change of $P_{f_1}$, $P_{f_2}$ after each iteration.}
\label{fig: 2.4}
\end{figure}
\newpage
\begin{figure}[H]
\centering
\includegraphics[width = 12cm , height = 18cm ]{2/24pd.eps}
\caption{Change of $P_d$ after each iteration.}
\label{fig: 2.5}
\end{figure}
\newpage

From the previous discussion of tolerance checking, since $k_1 > 0$ and $k_2 = 0$, if $ |c_1 - P_{f_1}| $ is smaller than the tolerance value, the algorithm stops. In this case, after the last iteration, we have $P_{f_1} = 0.2000$ and hence  $|P_{f_1} - c_1| = 0.000$ are within the prescribed tolerance value (which is $0.001$).  The algorithm stops after this iteration.   By using the exhaustive search method approached last section, we can verify the largest probability of detection under the constraint $\mathbf{P}_f \leq \mathbf{c}$ is $0.5629$, which is equal to the solution generated by the algorithm.
